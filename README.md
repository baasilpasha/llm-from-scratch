# LLM Implementation from Scratch

[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?logo=PyTorch&logoColor=white)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains implementations of core components for building large language models (LLMs) from scratch, following concepts from the book "Build a Large Language Model (From Scratch)" by Sebastian Raschka. The code covers tokenization, attention mechanisms, and GPT model implementation through the first four chapters of the book.

## Features

### Chapter 2: Tokenizing Text
- Custom text tokenization implementations
- Byte Pair Encoding (BPE) algorithm
- Vocabulary creation and management
- Text preprocessing pipelines

### Chapter 3: Attention Mechanisms
- Self-attention with and without trainable weights
- Multi-head attention implementation
- Causal attention masking
- Scaled dot-product attention
- Dropout regularization

### Chapter 4: GPT Model Implementation
- GPT model architecture
- Layer normalization implementation
- Feed-forward networks with GELU activation
- Residual connections
- Positional embeddings
- Token embedding layers

## File Structure
-
